{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UJLp8f-bjZfL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import LSTM\n",
        "from keras.layers.core import Dense, Activation\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import heapq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdzmcm3tkYGr",
        "outputId": "2006bdf4-c088-426d-c4a8-537f04426e82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsdE9pVrju0D",
        "outputId": "02e486de-b8a0-4e57-e8c0-e16866d530b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corpus length: 581888\n"
          ]
        }
      ],
      "source": [
        "text = open('/content/drive/MyDrive/datasets/NEXT WORD (1).txt').read().lower()\n",
        "print('corpus length:', len(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wyl9thw8ksna"
      },
      "outputs": [],
      "source": [
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "words = tokenizer.tokenize(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "s2fy1KYNkv5q"
      },
      "outputs": [],
      "source": [
        "unique_words = np.unique(words)\n",
        "unique_word_index = dict((c, i) for i, c in enumerate(unique_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlzFOvwokyg7",
        "outputId": "fbf1c6b5-3323-4ee5-86c8-bc92de17909f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['project', 'gutenberg', 's', 'the', 'adventures']\n",
            "of\n"
          ]
        }
      ],
      "source": [
        "WORD_LENGTH = 5\n",
        "prev_words = []\n",
        "next_words = []\n",
        "for i in range(len(words) - WORD_LENGTH):\n",
        "    prev_words.append(words[i:i + WORD_LENGTH])\n",
        "    next_words.append(words[i + WORD_LENGTH])\n",
        "print(prev_words[0])\n",
        "print(next_words[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dElxq1o1k2d5"
      },
      "outputs": [],
      "source": [
        "X = np.zeros((len(prev_words), WORD_LENGTH, len(unique_words)), dtype=bool)\n",
        "Y = np.zeros((len(next_words), len(unique_words)), dtype=bool)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "iLNrmmmTk79p"
      },
      "outputs": [],
      "source": [
        "for i, each_words in enumerate(prev_words):\n",
        "    for j, each_word in enumerate(each_words):\n",
        "        X[i, j, unique_word_index[each_word]] = 1\n",
        "    Y[i, unique_word_index[next_words[i]]] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VM5lu8F6k-k0",
        "outputId": "f3878aa5-a1c4-4faf-88e0-fdfccd572b9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[False False False ... False False False]\n"
          ]
        }
      ],
      "source": [
        "print(X[0][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ieZBu57elAFK"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(WORD_LENGTH, len(unique_words))))\n",
        "model.add(Dense(len(unique_words)))\n",
        "model.add(Activation('softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5Q3J-yelCox",
        "outputId": "b2a29ec9-9c5a-4265-d385-4a2fc089f4fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "519/519 [==============================] - 186s 353ms/step - loss: 6.4079 - accuracy: 0.0648 - val_loss: 7.1163 - val_accuracy: 0.0773\n"
          ]
        }
      ],
      "source": [
        "optimizer = RMSprop(learning_rate=0.01)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "history = model.fit(X, Y, validation_split=0.05, batch_size=200, epochs=1, shuffle=True).history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "imZDClzglF5B"
      },
      "outputs": [],
      "source": [
        "model.save('keras_next_word_model.h5')\n",
        "pickle.dump(history, open(\"history.p\", \"wb\"))\n",
        "\n",
        "model = load_model('keras_next_word_model.h5')\n",
        "history = pickle.load(open(\"history.p\", \"rb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5smmZLqTlJxh",
        "outputId": "ede18847-a0c5-4591-b475-dc5787995c82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "it\n",
            "is\n",
            "not\n",
            "a\n",
            "lack\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "def prepare_input(text):\n",
        "    x = np.zeros((1, WORD_LENGTH, len(unique_words)))\n",
        "    for t, word in enumerate(text.split()):\n",
        "        print(word)\n",
        "        x[0, t, unique_word_index[word]] = 1\n",
        "    return x\n",
        "prepare_input(\"It is not a lack\".lower())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "q1lZ9_hMlMEB"
      },
      "outputs": [],
      "source": [
        "def sample(preds, top_n=3):\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds)\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    return heapq.nlargest(top_n, range(len(preds)), preds.take)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "vsnKjh3glOWh"
      },
      "outputs": [],
      "source": [
        "def predict_completions(text, n=3):\n",
        "    if text == \"\":\n",
        "        return(\"0\")\n",
        "    x = prepare_input(text)\n",
        "    preds = model.predict(x, verbose=0)[0]\n",
        "    next_indices = sample(preds, n)\n",
        "    return [unique_words[idx] for idx in next_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nYbEpYW8VlX",
        "outputId": "b00928b5-463f-40d4-a7ea-65882e844431"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "correct sentence:  To Sherlock Holmes she is always _the_ woman.I have seldom heard him mention her under any other name.In his eyes she eclipses and predominates the whole of her sex. It was not that he felt any emotion akin to love for Irene Adler.\n",
            "Sequence:  to sherlock holmes she is\n",
            "to\n",
            "sherlock\n",
            "holmes\n",
            "she\n",
            "is\n",
            "next possible words:  ['the', 'a', 'to', 'that', 'me']\n"
          ]
        }
      ],
      "source": [
        "q =  \"To Sherlock Holmes she is always _the_ woman.I have seldom heard him mention her under any other name.In his eyes she eclipses and predominates the whole of her sex. It was not that he felt any emotion akin to love for Irene Adler.\"\n",
        "print(\"correct sentence: \",q)\n",
        "seq = \" \".join(tokenizer.tokenize(q.lower())[0:5])\n",
        "print(\"Sequence: \",seq)\n",
        "print(\"next possible words: \", predict_completions(seq, 5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuQkBo1SWKy7"
      },
      "source": [
        "#prediction by using Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "WZEA4e-EVi6I"
      },
      "outputs": [],
      "source": [
        "lexicon = {}\n",
        "\n",
        "def update_lexicon(current : str, next_word : str) -> None:\n",
        "    if current not in lexicon:\n",
        "        lexicon.update({current: {next_word: 1} })\n",
        "        return\n",
        "\n",
        "    options = lexicon[current]\n",
        "\n",
        "    if next_word not in options:\n",
        "        options.update({next_word : 1})\n",
        "    else:\n",
        "        options.update({next_word : options[next_word] + 1})\n",
        "\n",
        "    lexicon[current] = options"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "G4J-otB7V1Ty"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/datasets/project.txt', 'r') as dataset:\n",
        "    for line in dataset:\n",
        "        words = line.strip().split(' ')\n",
        "        for i in range(len(words) - 1):\n",
        "            update_lexicon(words[i], words[i+1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ZYNUPJp3XBYX"
      },
      "outputs": [],
      "source": [
        "# Adjust propability\n",
        "for word, transition in lexicon.items():\n",
        "    transition = dict((key, value / sum(transition.values())) for key, value in transition.items())\n",
        "    lexicon[word] = transition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9RMOodiXHto",
        "outputId": "448bffc3-580f-4ce2-9ada-f48007da5ef9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> wife\n",
            "wife so\n"
          ]
        }
      ],
      "source": [
        "# Predict next word\n",
        "line = input('> ')\n",
        "word = line.strip().split(' ')[-1]\n",
        "if word not in lexicon:\n",
        "    print('Word not found')\n",
        "else:\n",
        "    options = lexicon[word]\n",
        "    predicted = np.random.choice(list(options.keys()), p=list(options.values()))\n",
        "    print(line + ' ' + predicted)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}